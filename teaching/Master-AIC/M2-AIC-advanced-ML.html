<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
   <TITLE>Learning Theory and Advanced Machine Learning course</TITLE>
   <META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
   <META NAME="GENERATOR" CONTENT="User-Agent: Mozilla/3.0Gold (Macintosh; I; PPC)">
</HEAD>
<BODY>

<H2 ALIGN=CENTER><FONT COLOR="#1C74BD">Course Master AI: Learning Theory and Advanced Machine
    Learning</FONT></H2>

<CENTER><P>
<HR WIDTH="100%"><p></P></CENTER>
<CENTER>
<P><font face="Georgia, Times New Roman, Times, serif">Master 2 Informatique - Specialty AI (Course <em>Learning Theory and Advanced Machine Learning</em>)</font></P>
</CENTER>

<CENTER>
  <P><font face="Georgia, Times New Roman, Times, serif">Winter quarter 2024-2025</font></P>
</CENTER>

<CENTER>
  <P><font face="Georgia, Times New Roman, Times, serif">Teacher</font><font face="Georgia, Times New Roman, Times, serif">:&nbsp;&nbsp;&nbsp;<b> <font color="#000099">Antoine
          Cornu&eacute;jols</font></b>  </P>
</CENTER>

<!-- ==================================================================== -->
<P>
<HR WIDTH="100%">

<table width="100%" border="2" cellpadding = 3 cellspacing = 1 bordercolorlight="#CCCCCC" bordercolordark="#999999">
  <tr bgcolor="#FFCC99" align="center">
    <td width = 20%> <a href="#program">Program of the class</a> </td>
    <td width = 20%> <a href="#schedule">Tentative schedule of the classes</a> </td>
    <td width = 20%> <a href="#references">References</a> </td>
    <td width = 20%> <a href="#Etudebiblio">Articles to chose from</a> </td>
    <td width = 20%> <a href="#Projets">Projects</a> </td>
    <td width = 20%> <a href="#Stages">Internships</a> </td>
  </tr>
</table>

<p><font color="#CC3333">Last update: &nbsp;<!-- #BeginDate format:En2 -->12-February-2025<!-- #EndDate --> 
  </font></P>

<H3>Course Organization:</H3>

<ul>
        <li><b>&nbsp;Evaluation </b><!-- (<font color="#FF0000">ATTENTION nouveau</font>)</b>--> : 
        </li>
        <ul>
          <li>&nbsp;Quiz &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 5 * 6% = 30%</li>
           <li>&nbsp;Project replication of the results of a scientific paper + critical analysis  &nbsp;&nbsp;&nbsp;: 70%  &nbsp;&nbsp; (groups of 4 students)</li>
        </ul>
      <blockquote>
        <p>		</p>
      </blockquote>
      <li>&nbsp;<strong>Documents</strong> : </li>
  <ul>
    <li>&nbsp;Slides of the courses (see below in the course's schedule)</li>
<!--     <li>&nbsp;<a  href=M2-AIC_2017-2018-notes(app-avanc).pdf>Grades for the quizz 1, 2 and 3</a></li>
 -->
   </ul>
</ul>

<HR WIDTH="100%">

<!-- ==================================================================== -->
<H3>Outline of the course:  <a name="program" id="program"></a></H3>
<ul>
<!--    <div> <img src="Organisation_Course_AIC.pdf" align="center"  alt="Broad outline the course"  width="75%" height="50%" /></div> -->
    <li> Learning is about <strong>extrapolating</strong> predictions and/or regularities from limited data (even if sometimes there is an apparently very large data set) either in a stationary environment (<em>classical induction</em>) or in a changing one (<em>domain adaptation, transfer learning</em>). <br><br>
      This brings <strong>questions</strong> that will be core topics of the course:
    	<br> 
  <ol>
    </li>
    	<br>
    <li> In the case of stationary environments, there is a well developed theory called "the statistical theory of learning", but <b style="color:#3397AD";>what can be done when the environment changes</b> between the <em>training data</em> and the <em>testing one</em>?
    </li>
   <p> </p>	
     <li> Learning may imply several learning agents as in ensemble learning: <b style="color:#3397AD";>what information should they exchange?</b>
    </li>
    <p> </p>	
    <li> Learning may also imply several learning tasks, as in domain adaptation, transfer learning or online learning. Again, <b style="color:#3397AD";>what should be exchanged between contexts?</b>
    </li>
  </ol>
  <br>
  In this class, we will ask ourselves: 
    <ul>
    </li>
      <br>
    <li> <b style="color:#3397AD";>What is transferred in learning</b>, and specially in non stationary environments?
    </li>
   <p> </p> 
     <li> How to measure the <b style="color:#3397AD";>difficulty of learning an example</b>?
    </li>
    <p> </p>  
    <li> What is the best way to <b style="color:#3397AD";>organize a curriculum?</b> Or what is a <b style="color:#3397AD";>good teaching strategy</b>
    </li>
  </ul>

</ul>

<br>

<HR WIDTH="100%">

<!-- ==================================================================== -->
<H3>What you will acquire in the course:  <a name="program" id="program"></a></H3>
<ul>
  <ol>
<!--    <div> <img src="Organisation_Course_AIC.pdf" align="center"  alt="Broad outline the course"  width="75%" height="50%" /></div> -->
    <li> <font color="#000099">
      You will get <b>an understanding of the problem of induction</b>. You will ponder about the kind of guarantees that we can try to obtain as well as to which assumptions should we make and then be aware of when claiming theorems. In order to do so, you will look at the standard statistical learning theory and examine extensions to it.</font> 
    </li>
      <br>
    <li>  <font color="#000099"> You will gain a better appreciation of what is involved in <b>ensemble methods</b> and, in particular, what kind of communication there can be between the agents and how their advices can be combined in order to get a decision.</font>
    </li>
   <p> </p> 
     <li>  <font color="#000099"> You will know what are the problems that face <b>"out of distribution" learning</b>, and get knowledge about some recent directions of research on this problem.</font>
    </li>
    <p> </p>  
    <li>  <font color="#000099"> As part of a project group of students, you will <b>study in depth a recent research paper</b> and try to <b>replicate its results</b>.</font>
    </li>
   <p> </p> 
    <li>  <font color="#000099">As part of a project group of students, you will exercice a <b>critical apparaisal of a research paper</b>.</font>
    </li>
 <!--   <p> </p>  
   <li>  <font color="#000099">You will beneficiate from the results obtained by other groups of students on other research papers.</font>-->
    </li>
  </ol>
  <br>


<!-- ==================================================================== -->

<HR WIDTH="100%">

<table width="100%" border="1">
  <tr> 
    <td width="14%" bgcolor="#CCCCFF"><b><font color="#990000">Dates :</font></b></td>
    <td width="52%" bgcolor="#CCCCFF"><b><font color="#990000">Topics &nbsp;&nbsp;(tentative schedule)</font></b></td>
    <td width="34%" bgcolor="#CCCCFF"><b><font color="#990000">References, exercises and
    homeworks</font></b></td>
  </tr>


<!-- ==================================================================== -->
<!-- ==================================================================== -->
<H3>Tentative schedule:  <a name="schedule" id="schedule"></a></H3>

  <tr> 
    <td width="14%" bgcolor="#D4FCFF"> 
      <p><b>09-01-2025</b></p>
      <p align="right">09h00 - 12h15  (<font color="#FF0000">Salle B-107</font>)</p>
    </td>
    <td width="52%" bgcolor="#FFFFFF"> 
      <p align="right">(Antoine Cornu&eacute;jols)</p>
      <p> <b>&nbsp;&nbsp;&nbsp;- </b> Introduction to the course. Out-of-Distribution learning. </p>
        <p> <b>Central question of the course</b>: <em>What makes you believe that what you have learned so far from (source) training examples or task will be relevant to new (target) examples or tasks? </em> </p>
      <p> <a href="https://seafile.agroparistech.fr/f/15e94ce7121f4bafafb8/"><font color="#3397AD">Examination of <b>classical induction</b></font></a></p>      
       <p><b>&nbsp;&nbsp;&nbsp;- </b>The statistical theory of learning for a stationary world. (The In-Distribution assumption)

    <br>      
        
    <td bgcolor="#FEEAE0">
  </tr>
<!-- ==================================================================== -->
<!-- ==================================================================== -->
  <tr> 
    <td width="14%" height="120" bgcolor="#D4FCFF"> 
      <p><b>16-01-2025</b></p>
      <p align="right">09h00 - 12h15  (<font color="#FF0000">Salle B-107</font>)</p>
    </td>

    <td width="52%" bgcolor="#FFFFFF"> 
      <p align="right">(Antoine Cornu&eacute;jols)</p>
      <p><a href="https://seafile.agroparistech.fr/f/e506d1c75c4e4fd7a656/"><font color="#3397AD">Out-of-Distribution learning <b>for the sake of learning better</b> or with less (annotated) examples</font></a> </p>
      <ul>
        <li> 
          <p><b>Semi-supervised</b> learning </p> 
        </li>
        <li> 
          <p>Learning when classes are severely <b>unbalanced</b> </p>
        </li>
        <li> 
          <p>Learning from <b>positive examples only</b> </p>
        </li>
        <li> 
          <p><b>Active</b> learning </p>
        </li>
      </ul>

      <p> <b><a href="https://seafile.agroparistech.fr/f/a6f1c58da20b464d8604/"><font color="#3397AD">O.O.D. through multiple agents</font></a></b> </p>
      <ul>
        <li>
          <p>Co-training, self-training and tri-training </p>  
       </li> 
        <li>
          <p>Boosting (after a reminder on SVMs)</p>  
       </li> 
     </ul>
 
       <br>
 
 <!--       <p><b>&nbsp;&nbsp;&nbsp;- </b>The statistical theory of learning / The no-free-lunch theorem / Another perspective: Explanation-Based-Learning / What kind of validation? -->
    </td>
         
    <td width="34%" bgcolor="#FEEAE0">
          <p>&#149; Quiz No 1</p>
<!--          <p>&#149; <a href="exercice-boosting.pdf">Exercise on boosting</a> </p> 
-->
<p>&#149; 
         

    </td>

  </tr>
<!-- ==================================================================== -->
<!-- ==================================================================== -->
  <tr> 
    <td width="14%" height="120" bgcolor="#D4FCFF"> 
      <p><b>23-01-2025</b></p>
      <p align="right">09h00 - 12h15  (<font color="#FF0000">Salle B-107</font>)</p>
    </td>

    <td width="52%" bgcolor="#FFFFFF"> 
      <p align="right">(Antoine Cornu&eacute;jols)</p>
      <p> <b><a href="https://seafile.agroparistech.fr/f/a6f1c58da20b464d8604/"><font color="#3397AD">O.O.D. through multiple agents</font></a></b> (follow up)</p>
      <ul>
        <li>
          <p>Boosting (after a reminder on SVMs)</p>  
       </li> 
        <li>
          <p>Blending </p>  
       </li> 
     </ul>
      <p> <b><a href="https://seafile.agroparistech.fr/f/6b48e3f1220e49e09f01/"><font color="#3397AD">Domain adaptation and tracking</font></a></b> </p>
     <ul>
        <li>
          <p>Domain adaptation (co-variate shift) </p>  
       </li> 
        <li>
          <p>Tracking: an intriguing idea. Domain adaptation without domain change!</p>  
       </li> 
     </ul>

      <p> <b><a href="https://seafile.agroparistech.fr/f/1f11ce0170dd4afb9c8b/"><font color="#3397AD">Transfer learning</font></a></b> </p>
      <p>What can be transferred:</p>
      <ol>
        <li>
          <p><b>Representation</b></p>
        </li>
        <li>
          <p>Underlying <b>regularities</b> supposedly more stable among learning tasks</p>
        </li>
        <li>
          <p>The <b>decision</b> function</p>
        </li>
        <li>
          <p><b>Others</b>: the loss function, the optimizer, ... </p>
        </li>
      </ol>
      <p>(1) <b>Learning a representation</b></p>

      <ul>
        <li>
          <p>Fine tuning</p>
        </li>
        <li>
          <p>Self-supervised learning</p>
        </li>
        <li>
          <p>Contrastive learning</p>
        </li>
        <li>
          <p>Foundation models. Prompting. Zero-shot learning</p>
        </li>
      </ul>

       <br>
 
 <!--       <p><b>&nbsp;&nbsp;&nbsp;- </b>The statistical theory of learning / The no-free-lunch theorem / Another perspective: Explanation-Based-Learning / What kind of validation? -->
    </td>
         
    <td width="34%" bgcolor="#FEEAE0">
          <p>&#149; Quiz No 2</p>
<!--          <p>&#149; <a href="exercice-boosting.pdf">Exercise on boosting</a> </p> 

<p>&#149; <a href="Tr-collaborative_learning-2023-24-v2.pdf">Slides of the class</a>
          <p></p>
-->
    </td>

  </tr>
<!-- ==================================================================== -->
<!-- ==================================================================== -->
  <tr> 
    <td width="14%" height="120" bgcolor="#FFCCCC"> 
      <p><b>30-01-2025</b></p>
      <p align="right">09h00 - 12h15  (<font color="#FF0000">Salle B-107</font>)</p>
    </td>
    
    <td bgcolor="#FDC9AF"><p align="right">(Antoine Cornu&eacute;jols)</p>
      <p></p>
      <p> <center><b>No class!! </b></center></p>   
   <br>      
      <p align=right> 
      </p>
    </td>

    <td bgcolor="#FFCCCC">
   </td>

  </tr>
<!-- ==================================================================== -->
<!-- ==================================================================== -->
  <tr> 
    <td width="14%" bgcolor="#D4FCFF"> 
      <p><b>13-02-2025</b></p>
      <p align="right">09h00 - 12h15  (<font color="#FF0000">Salle B-107</font>)</p>
    </td>
    
    <td width="52%"> 
      <p align="right">       (Antoine Cornu&eacute;jols)</p>
      <p> <b><a href="https://seafile.agroparistech.fr/f/1f11ce0170dd4afb9c8b/"><font color="#3397AD">Transfer learning</font></a></b> </p>

      <p>(2) <b>Discovering underlying regularities</b> to transfer</p>

      <ul>
        <li>
          <p>Domain adversarial learning</p>
        </li>
        <li>
          <p>Invariant Risk Minimization</p>
        </li>
       </ul>

      <p>(3) Transfering the <b>decision function</b> instead of the representation (<a href="https://seafile.agroparistech.fr/f/7c55ebd3df7d4fdd9247/">slides</a>)</p>

      <ul>
        <li>
          <p><b>TransBoost</b></p>
          <ul><b>- </b>   What are the <strong>ingredients</strong> for a successful (detrimental) transfer? </ul>
          <ul><b>- </b>   What <strong>role for the source</strong>? Role and measure of similarity between source and target in TL? </ul>
        </li>
        <li>
          <p><b>Analogy</b> making and ordering effects. The MDLp (Minimum Description Length Problem)</p>
        </li>
        <li>
          <p><b>Cognitive tunnel effect</b></p>
          <ul><b>- </b>   How to transfer from one conceptual domain to another one. Is there a better ordering?</ul> 
        </li>
       </ul>

      <p> <b><a href="https://seafile.agroparistech.fr/f/52b821640b2e438ebb1e/"><font color="#3397AD">Learning from a teacher</font></a></b> </p>
      <ul>
        <li>
          <p> <b>LUPI</b> (Learning Using Privileged Information): not the same input space X between teacher and learner.</p>
        </li>
        <li>
          <p><b>Distillation</b>. Not the same H.</p>
        </li>
        <li>
          <p><b>Prompting</b> a foundation model.</p>
        </li>
      </ul>

 
<!--
      <p>&#149; <a ><b>Co-learning</b></a>. Having independent and complementary views.</p>      
     <p>&#149; <a ><b>Unsupervised Collaborative learning</b></a> and the question of which methods are to be combined in order to get the best collaboration. Which lessons here?</p>      
     <p>&#149; <a > <b>The LUPI framework</b></a>. Idea. Learning using a given input space, and being tested using another one. </p>
      <p><b>&nbsp;&nbsp;&nbsp;- </b> <a >Illustration: Early classification of time series.</a></p>
     <p>&#8226; <a ><b>Domain adaptation</b></a>. </p>
      <p>&#149; <a ><b>Co-learning</b></a>. Having independent and complementary views.</p>      
      <p>&#149; <a href="Tr-collaborative_learning-2021-22-v2.pdf"><b>Co-learning</b></a>. Having independent and complementary views.</p>
           <p>&#149; <a href="Tr_collaborative_clustering.pdf"><b>Unsupervised Collaborative learning</b></a> and the question of which methods are to be combined in order to get the best collaboration. Which lessons here?</p>      
     <p>&#149; <a href="Tr_Course_LUPI_ECTS-2021-22-v1.pdf"> <b>The LUPI framework</b></a>. Idea. Learning using a given input space, and being tested using another one. </p>
      <p><b>&nbsp;&nbsp;&nbsp;- </b> <a >Illustration: Early classification of time series.</a></p>
     <p>&#8226; <a href="Tr_Domain_Adaptation_21-22.pdf"><b>Domain adaptation</b></a>. </p>
   -->
    <br>      
    </td>
    <td width="34%" bgcolor="#FEEAE0">
      <p>&#149; Quiz No 3</p>
      <p> <a href="https://seafile.agroparistech.fr/f/c0da1fe0e7534d618c08/">CORNU&Eacute;JOLS, Antoine. <b>Some thoughts about transfer learning. What role for the source domain?</b>. <em>International Journal of Approximate Reasoning</em>, 2024, vol. 166, p. 109107.</a> 
     </p>
      <p><em> <a href="https://eprints.bbk.ac.uk/id/eprint/44038/1/KD_Survey-arxiv.pdf">Gou, J. and Yu, B. and Maybank, Stephen and Tao, Da. (2021) <b>Knowledge distillation: a survey</b>. International Journal of Computer Vision 129, pp.1789-1819.</a></em> 
     </p>
      <p><em> <a href="https://proceedings.mlr.press/v97/phuong19a/phuong19a.pdf">PHUONG, Mary et LAMPERT, Christoph. <b>Towards understanding knowledge distillation</b>. In : International conference on machine learning. PMLR, 2019. p. 5142-5151.</a></em> 
     </p>

    </td>

  </tr>
<!-- ==================================================================== -->
<!-- ==================================================================== -->
  <tr> 
    <td width="14%" height="120" bgcolor="#D4FCFF"> 
      <p><b>14-02-2025</b></p>
      <p align="right">09h00 - 12h15  (<font color="#FF0000">Salle B-107</font>)</p>
    </td>
    
    <td bgcolor="#FFFFFF"><p align="right">(Antoine Cornu&eacute;jols)</p>
      <p></p>
      <p> <b><font color="#3397AD">Curriculum learning</font></b> </p>

      <ul>
        <li>
          <p> <b>Sequence of tasks</b>. How to organize them? </p>
        </li>
        <li>
          <p>How to <b>measure the difficulty</b> of new examples</p>
        </li>
        <li>
          <p><b>Catastrophic forgetting</b>. </p>
        </li>
        <li>
          <p><b>Coaching</b>. Not the same gain function.</p>
        </li>
        <li>
          <p><b>Curriculum</b>: the space of tasks and the space of biases.</p>
        </li>
      </ul>


   <br>      
      <p align=right> 
      </p>
    </td>

    <td bgcolor="#FEEAE0">
      <p>&#149; Quiz No 4</p>
      <p> <em> <a href="https://arxiv.org/pdf/2011.00613.pdf">(ICML-2021)-An Information-Geometric Distance on the Space of Tasks</a></em>. 
     </p>
      <p> <em> <a href="https://arxiv.org/pdf/2107.04384.pdf">(ICML-2021)-Continual Learning in the Teacher-Student Setup: Impact of Task Similarity</a></em>. 
      </p>
     </td>

  </tr>
<!-- ==================================================================== -->
<!-- ==================================================================== -->
  <tr> 
    <td width="14%" height="120" bgcolor="#D4FCFF"> 
      <p><b>20-02-2025</b></p>
      <p align="right">09h00 - 12h15  (<font color="#FF0000">Salle B-107</font>)</p>
    </td>
    
    <td bgcolor="#FFFFFF"><p align="right">(Antoine Cornu&eacute;jols)</p>
      <p></p>
     <p> <b><font color="#3397AD">On-line learning. A succession of small transfer steps</font></b> </p>
      <a href="Tr_Curriculum_learning-23-24(1-123).pdf">Slides of the class</a>
      <ul>
        <li>
          <p> A <b>theoretical</b> perspective. <br>
          Can we still guarantee a performance level even against an adversary who controls the target concept and can change it arbitrarily?</p>
        </li>
       <li>
          <p> <b>Empirical approaches</b>. </p>
        </li>
       </ul>

      <p> <b><font color="#3397AD">General conclusions</font></b> </p>


    <br>      
      <p align=right> 
      </p>
    </td>

    <td bgcolor="#FEEAE0">
      <p>&#149; Quiz No 5</p>
      <p>&#149; <b>Final reports</b> on the projects are due</p>
    </td>

  </tr>
<!-- ==================================================================== -->
<!-- ==================================================================== -->
  
  
  
<!-- ==================================================================== -->
<!-- ==================================================================== -->
</table>
<H3>&nbsp;</H3>
<h3>References and web sites: <a name="references" id="references"></a></h3>
<ul>
  <li>&nbsp; Barra V. &amp; Cornu&eacute;jols A. &amp; Miclet L.: <u><font color="#000099">Apprentissage artificiel. Concept et algorithmes. De Bayes et Hume au Deep Learning</font></u>. Eyrolles, 2021 (4&egrave;me &eacute;dition)<br>
  &nbsp; (A very comprehensive book on Machine Learning. In French.)</li>
</ul>
<p>
<ul>
  <li>&nbsp; Torralba A. &amp; Isola Ph. &amp; Freeman W.: <u><font color="#000099">Foundations of Computer Vision</font></u>. MIT Press, 2024.<br>
  &nbsp; (Speaks mainly of computer vison obviously. But an incredible synthesis of learning problems and methods. <br> &nbsp; Highly recommended.)</li>
</ul>

<p><strong>Books on Machine Learning in general</strong>:</p>
<ul>
  <li> &nbsp;Barber D. :<u> Bayesian Learning and Machine Learning</u>. Cambridge
    University Press,
    2012.<br>
    &nbsp;(A Bayesian perspective on Machine Learning.)
</ul>
<ul>
  <li> &nbsp;Bishop Ch. :<u> <font color="#000099">Deep Learning. Foundations and Concepts</font></u>. MIT Press,
    2024.<br>
    &nbsp;(Quite pedagogical.)
</ul>
<ul>
  <li> &nbsp;Duda, Hart &amp; Stork :<u> Pattern classification</u> (2nd &eacute;d.).
    Wiley-Interscience, 2001.<br>
&nbsp;(Oriented towards Pattern Recognition. Very good graphics. Still a reference.)
</ul>
<ul>
  <li>&nbsp;Flach P. : <u>Machine Learning. The art and science of algorithms
        that make sense of data</u>.
      Cambridge University Press, 2012.<br>
&nbsp;(A good introductory book on Machine Learning. Quick on some subjects and
does not really cover data mining. But a very original and often quite illuminating perspective.)</li>
</ul>
<!--
<ul>
  <li>&nbsp;Hand, Mannila &amp; Smyth : <u>Data mining</u>. Springer, 2001.<br>
    &nbsp;(Livre assez complet, mais n&eacute;cessairement plus superficiel car couvrant aussi la fouille de donn&eacute;es)</li>
</ul>
-->
<ul>
  <li>&nbsp;Hastie, Tibshirani &amp; Friedman : <u>The elements of statistical
       learning. Data mining, inference and prediction</u>. Springer, (2nd Ed.
       2009).<br>
    &nbsp;(A bit demanding, but very informative. The form is outstanding.)</li>
</ul>
<ul>
  <li>&nbsp;Haykin S. : <u>Neural netwoks. A comprehensive foundation</u>. Prentice 
    Hall, 1999.<br>
  &nbsp;(An incredible source of information)</li>
</ul>
 <ul>
  <li>&nbsp;Haykin S. : <u>Neural netwoks and Learning Machines</u>. Prentice
    Hall, 2008.<br>
&nbsp;(Different from the 1999 edition, and yet still remarkable, and worth
the reading.)
  </li>
</ul>
<!--
<ul>
  <li> &nbsp;Mitchell T. : <u>Machine Learning</u>. McGraw Hill, 1997. <br>
    &nbsp; (A good introductory book, but outdated on many subjects.)</li>
</ul>
-->
<!--
<ul>
  <li> &nbsp;Marsland S. : <u>Machine Learning. An algorithmic perspective</u>.
    CRC Press, 2009. <br>
    &nbsp; (An introductory book, rather superficial, with bits of code in Python.)</li>
</ul>
-->
<ul>
  <li> &nbsp;Murphy K. : <u>Machine Learning. A probabilistic perspective</u>.
    MIT Press, 2012. <br>
&nbsp; (The title says it all. A book incredibly comprehensive from a Bayesian
perspective.)</li>
</ul>
<ul>
  <li> &nbsp;Murphy K. : <u><font color="#000099">Probabilistic Machine Learning. A introduction</font></u>.
    MIT Press, 2022. <br>
&nbsp; (The new edition, first of two volumes.)</li>
</ul>
<ul>
   <li> &nbsp;Murphy K. : <u><font color="#000099">Probabilistic Machine Learning. Advanced topics</font></u>.
    MIT Press, 2022. <br>
&nbsp; (The new edition, second of two volumes. Lots of recent works. A little bit of a patchwork.)</li>
</ul>
<ul>
   <li> &nbsp;Sejnowski T. : <u><font color="#000099">The deep learning revolution</font></u>.
    MIT Press, 2018. <br>
&nbsp; (Provides a personal historical perspective. By someone who should have shared the 2024 Nobel Prize.)</li>
</ul>
<ul>
   <li> &nbsp;Zou Zhi-Hua. : <u><font color="#000099">Machine Learning</font></u>.
    Springer, 2016. <br>
&nbsp; (A very good introductory book)</li>
</ul>
<p><strong></strong></p>

<p><strong>Books on specific topics</strong>:</p>
<ul>
  <li>&nbsp;Bishop C. : <u>Neural networks for pattern recognition</u>. Clarendon 
    Press, 1995.<br>
&nbsp;(A good introductory book. Oriented towards connectionism but discusses
many important issues in Machine Learning as well.)
  </li>
</ul>
<ul>
  <li>&nbsp;Cristianini N. &amp; Shawe-Taylor J. : <u>Support Vectors Machines
      and other kernel-based learning methods</u>. Cambridge University Press,
      2000.</li>
</ul>
<ul>
  <li>&nbsp;Webb A. : <u>Statistical pattern recognition</u>. Wiley, (3rd. Ed.,
    2011).<br>
    &nbsp;(A good introductory book that deserves to be better known.) <br>
  </li>
</ul>
<!-- <p><strong>Web sites</strong> :</p>
<ul>
  <li>&nbsp;<a href="http://www.ai.univie.ac.at/oefai/ml/ml-resources.html">http://www.support-vector.net</a><br>
    &nbsp; (Un point d'entr&eacute;e sur les machines &agrave; vecteurs de support)</li>
</ul>
<ul>
  <li>&nbsp;<a href="http://www.ai.univie.ac.at/oefai/ml/ml-resources.html">http://www.ai.univie.ac.at/oefai/ml/ml-resources.html</a><br>
    &nbsp; (Un site recensant de tr&egrave;s nombreuses ressources en Apprentissage 
    Artificiel)</li>
</ul>
-->

<!-- ==================================================================== -->
<!-- ==================================================================== -->
<HR WIDTH="100%">
<h3><font color="#000099">Guidelines for the projects<a name="Projets"></a>:</font></h3>
<!-- ==================================================================== -->

This year, your assignment in the projects is to choose an article from the list provided below, understand it, and, most importantly, <strong>try to replicate the experiments</strong>.

<ul></ul>
<br>
Rules are as follows:

<ul>
   <li> Projects are conducted by <strong>team of 4 students</strong>. Team members are responsible for decompose the work so that all members contribute meaningfully to the project.  
   </li>
</ul>
   
<br>
The following deliverables are expected:

<ol>
   <li> Before (<strong><em> <font color="#990000">January 16th 2025 </font></em></strong>), the <strong>choice of the paper</strong> and the names of the members of the team.
   </li>
   <li> (<strong><em> <font color="#990000">February, 20th 2025 </font></em></strong>) <strong> Final report </strong> : 10 pages (strict. Reports longer than 10 pages will not be read!)
   </li>
</ol>

<br>
The intermediary report and the final report must be turned down in the ICML (Int. Conf. on Machine Learning) format for <a href="http://icml.cc/2016/?page_id=151">papers</a>.

<br>
<br>
The deliverables will be evaluated taking into account:

<br>
<br>
<ul>
   <li>
   	The <strong>clarity of the analysis of the chosen article</strong>. The paper should briefly present, in no more than two to four pages (out of the 10 allotted), the problem studied in the article, the main ideas and statements presented in the paper. The remaining pages will present the experiments carried out by the team, the possible difficulties, the results obtained and a comparison with the results presented by the authors of the article used as a basis for the project.
   </li>
   <br>
   <li> 
   	The <strong>rigor</strong> and the extensive character of the analysis and/or the experiments carried out. A project that really answers the questions and possible doubts of the reviewers on the interest of the method and on the announced performances will obtain a higher score.
   </li>
   <br>
   <li> 
   	Expression, <strong>clarity of explanation</strong> and quality of exposition. Reports can be written in French or English, as long as they are clear and well written.
   </li> 
</ul>

<br>


<HR WIDTH="100%">

<strong>List of the papers to chose from</strong><a name="#Etudebiblio" id="Etudebiblio"></a> (all recent ones)
      <br> 
<br>

  <ol>
       <li> <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="https://arxiv.org/pdf/2007.01434.pdf">(2020) In search of lost domain generalization</a></em>.  
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b><em> <a href="https://seafile.agroparistech.fr/f/ce4e9558d2b1442a8857/">(ICLR-2024) Self-Supervised Dataset Distillation for Transfer Learning</a></em>. 
     </li>
<p>
       <li> <b>New</b> <em> <a href="https://seafile.agroparistech.fr/f/d4a139ea3b5f43ddb8de/">(ICLR-2024) Transferring Learning Trajectories of Neural Networks</a></em>. 
     </li>
<p>
       <li> <b></b> <em> <a href="https://proceedings.mlr.press/v202/liu23ap/liu23ap.pdf">(ICML-2023) Taxonomy-Structured Domain Adaptation</a></em>. 
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="https://arxiv.org/abs/2302.12091 ">(ICML-2023) Random Teachers are Good Teachers</a></em>. 
     </li>
<p>
       <li> <b></b> <em> <a href="https://arxiv.org/pdf/2306.09890.pdf">(ICML-Workshop-2023) Studying Generalization on Memory-Based Methods in Continual Learning</a></em>. 
     </li>
<p>
       <li> <b>New</b> <em> <a href="https://seafile.agroparistech.fr/f/7f85c3bfa51e4b50b267/">(ICLR-2024) Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP</a></em>. 
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b><em> <a href="https://openreview.net/pdf?id=Oa9RlXNggGy">(Neurips-2022) Does knowledge distillation really works?</a></em>. 
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="http://proceedings.mlr.press/v139/lee21e/lee21e.pdf">(ICML-2021) Continual learning in the teacher-student setup: impact of task similarity</a></em>. 
     </li>
<p>
       <li> <b>Partly taken</b> <em> <a href="https://arxiv.org/pdf/2002.08546.pdf">(ICML-2020) Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</a></em>. 
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="https://arxiv.org/pdf/2007.12684.pdf">(ICCV-2021) Deep co-training with task decomposition for semi-supervised domain adaptation</a></em>. 
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="https://seafile.agroparistech.fr/f/7f3089f7a27241f4894f/">(ICML-2024) Transferring Knowledge From Large Foundation Models to Small Downstream Models</a></em>. 
     </li>
<p>
       <li> <b><P>Partly taken</P></b> <em> <a href="https://seafile.agroparistech.fr/f/33a561fcef034256a522/">(ICML-2024) Pseudo-Calibration-Improving Predictive Uncertainty Estimation in Unsupervised Domain Adaptation</a></em>. 
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="https://seafile.agroparistech.fr/f/3971ab3ca4bd4175a5a9/">(ICML-2024) Soft_Contrastive_Learning</a></em>. 
     </li>
<p>
       <li> <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="https://seafile.agroparistech.fr/f/ff552f663abf41e59b69/">(ICML-2024) Let Go of Your Labels with Unsupervised Transfer</a></em>. 
     </li>
<p>
       <li> <b>New</b> <em> <a href="https://seafile.agroparistech.fr/f/2aa53310fa794b5db619/">(IJCAI-2024) Alleviating Imbalanced Pseudo-label Distribution- Self-Supervised Multi-Source Domain Adaptation with Label-specific Confidence</a></em>. 
     </li>
<p>
       <li> <b>New</b> <em> <a href="https://seafile.agroparistech.fr/f/51c7779d89e240568822/">(IJCAI-2024) Reconfgurability-Aware Selection for Contrastive Active Domain Adaptation</a></em>. 
     </li>
<p>
       <li>  <b><font color="#FF0000"> <b>Taken </font></b> <em> <a href="https://arxiv.org/pdf/2202.00155">(ICLR-2022) Fortuitous forgetting in connectionist networks</a></em>. 
     </li>
<p>
    </ol>

   
 

<p>
</p>

<!-- ==================================================================== -->
<!-- ==================================================================== -->
<HR WIDTH="100%">
<H3>Projects chosen by the students:</H3>
  <table width="100%" border="0"  cellspacing="12">
  <tr> 
    <td width="25%" bgcolor="#CCCCFF"><b><font color="#990000">Names :</font></b></td>
    <td width="75%" bgcolor="#CCCCFF"><b><font color="#990000">Projects :</font></b>
  </tr>
<!-- ==================================================================== -->


  <tr>
     <td>
        <li>Student-1</li>
        <li>Student-2</li>
        <li>Student-3</li>
        <li>Student-4</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a >(Conference) Paper title</a></em>. 
        </li>
        <li> (final report: <font color="#3fd859"> <b>YES</b> </font>)</li>
      </td>
  </tr>
  
  <tr>
     <td>
        <li>Biruk Abere Ambaw</li>
        <li>Emirhan Bilgi&ccedil;</li>
        <li>Amruth Srivathsan</li>
        <li>Aaron Weissberg</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://openreview.net/pdf?id=Oa9RlXNggGy">(Neurips-2022) Does knowledge distillation really works?</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>

  <tr>
     <td>
        <li>Ying Lai</li>
        <li>Huichi Kuo</li>
        <li>Niloofar Sahebalzamany</li>
        <li>Student-4</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://arxiv.org/abs/2302.12091 ">(ICML-2023) Random Teachers are Good Teachers</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>

  <tr>
     <td>
        <li>Titouan Chambe</li>
        <li>Maiwen Demeulle</li>
        <li>Antoine Loth</li>
        <li>Titouan Chambe</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://seafile.agroparistech.fr/f/7f3089f7a27241f4894f/">(ICML-2024) Transferring Knowledge From Large Foundation Models to Small Downstream Models</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>

  <tr>
     <td>
        <li>Adrien Bouchet</li>
        <li>Vincent Fourmigu&eacute;</li>
        <li>Elise Roth</li>
        <li>Timoth&eacute;e Sanchez</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://seafile.agroparistech.fr/f/3971ab3ca4bd4175a5a9/">(ICML-2024) Soft_Contrastive_Learning</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>

   <tr>
     <td>
        <li>Lisa Achard</li>
        <li>Manon Chartrin</li>
        <li>Sovanleng Ly</li>
        <li>Nabil Meridja</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="http://proceedings.mlr.press/v139/lee21e/lee21e.pdf">(ICML-2021) Continual learning in the teacher-student setup: impact of task similarity</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  
   <tr>
     <td>
        <li>Linh Dinh</li>
        <li>M&eacute;lodie Fleury</li>
        <li>Nicolas Geffroy</li>
        <li>Mat&eacute;o Petitet</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://arxiv.org/pdf/2202.00155">(ICLR-2022) Fortuitous forgetting in connectionist networks</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  
   <tr>
     <td>
        <li>Camille De Amorim</li>
        <li>Mapathe Faye</li>
        <li>Jaffar Gura</li>
        <li>Negin Heidarifard</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://arxiv.org/pdf/2007.01434.pdf">(2020) In search of lost domain generalization</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  
   <tr>
     <td>
        <li>Jose Espinosa</li>
        <li>Martin Masson</li>
        <li>Dana Zlotenko </li>
        <li>Student-4</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://seafile.agroparistech.fr/f/ce4e9558d2b1442a8857/">(ICLR-2024) Self-Supervised Dataset Distillation for Transfer Learning</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  
   <tr>
     <td>
        <li>Jhonatan Felix</li>
        <li>Aristide Lauront</li>
        <li>Rapha&euml;l Rubrice</li>
        <li>Student-4</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://seafile.agroparistech.fr/f/33a561fcef034256a522/">(ICML-2024) Pseudo-Calibration-Improving Predictive Uncertainty Estimation in Unsupervised Domain Adaptation</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  
   <tr>
     <td>
        <li>No&eacute;mie Bozier</li>
        <li>Nathan Carr&eacute;</li>
        <li>Anakim Gualdoni</li>
        <li>Georges Hanna</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://seafile.agroparistech.fr/f/ff552f663abf41e59b69/">(ICML-2024) Let Go of Your Labels with Unsupervised Transfer</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  
   <tr>
     <td>
        <li>Tim&eacute;o Hennebelle</li>
        <li>Martin Simonovietz</li>
        <li>Student-3</li>
        <li>Student-4</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://arxiv.org/pdf/2007.12684.pdf">(ICCV-2021) Deep co-training with task decomposition for semi-supervised domain adaptation</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  
   <tr>
     <td>
        <li>Younes Djemmal</li>
        <li>Lokman Ouali</li>
        <li>Student-3</li>
        <li>Student-4</li>
     </td>
      <td bgcolor="F3F3F3">      
        <li> <em> <a href="https://arxiv.org/pdf/2002.08546.pdf">(ICML-2020) Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</a></em>. 
        </li>
        <li> (final report: <font color="#FF0000"> <b>NO </font>)</li>
      </td>
  </tr>
  


 

<!-- ==================================================================== -->
  </table>
</UL>
<p><br>
  
  
<!-- ==================================================================== -->
<!-- ==================================================================== -->
<HR WIDTH="100%">
<h3><font color="#000099">Propositions for internships<a name="Stages"></a> :</font></h3>
<!-- ==================================================================== -->

<!-- ==================================================================== -->
</table>
<!-- 
<p>&nbsp;</p>
<HR WIDTH="100%">
<H3>Propositions de stages<a name="Stages"></a> :</H3>
-->
<UL>
  <blockquote>
    <p>&nbsp;</p>
  </blockquote>
  <table width="852" border="0" cellspacing="12">
    <!--DWLayoutTable-->
  <!--  <tr>
       <td bgcolor="#F3F3F3"><a href="../STAGES-Master/stage-Orange-flux-data-2018.pdf">Apprentissage &agrave; partir de flux de donn&eacute;s : d&eacute;tection d'anomalies</a></td>
      <td>Orange Labs / AgroParisTech</td>
    </tr>
    <tr>
      <td bgcolor="E9F9F6"><a href="../STAGES-Master/Sujet-Stage Arkyan Classifier Brevet.pdf">Classification automatique de brevets</a></td>
      <td>Arkian (Montpellier)</td>
    </tr>
    <tr>
      <td bgcolor="F3F3F3"><a href="../STAGES-Master/sujet-stage-WP1PhDStage-agro-SHIFT.pdf">Personnalized recommendation system for food choices</td>
      <td>AgroParisTech / Danone</td>
    </tr>
   -->
   <tr>
      <td bgcolor="E9F9F6">sujet</td>
      <td>Institution</td>
    </tr>
    <tr>
      <td bgcolor="F3F3F3">sujet</td>
      <td><p>Institution</p>
      </td>
    </tr>
    <tr>
      <td bgcolor="F3F3F3">sujet</td>
      <td>Institution</td>
    </tr>
    <tr>
      <td bgcolor="F3F3F3">sujet</td>
      <td>Institution</td>
    </tr>
    <tr>
      <td bgcolor="F3F3F3">sujet</td>
      <td>Institution</td>
    </tr>
  </table>
</UL>
<p><br>
</p>
<P>
<!-- ==================================================================== -->



<p>&nbsp;</p>
<p>&nbsp;</p>
</BODY>
</HTML>
